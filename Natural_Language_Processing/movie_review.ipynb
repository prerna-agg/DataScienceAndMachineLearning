{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Reviews has 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the file id of 5th document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'neg/cv005_29357.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "movie_reviews.fileids()[5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access word written in a review using the review's file id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['capsule', ':', 'in', '2176', 'on', 'the', 'planet', ...]"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids()[5]) ## accessing the 5th review and array of words is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid),category)) ## storing a tuple for each document\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(['shakespeare', '.', 'you', 'hardly', 'understood', ...], 'pos'),\n",
       " (['synopsis', ':', 'an', 'aging', 'master', 'art', ...], 'neg'),\n",
       " (['take', 'two', 'old', 'and', 'dying', 'men', ',', 'a', ...], 'neg'),\n",
       " (['in', 'chocolat', ',', 'a', 'chocolate', 'shop', ...], 'pos'),\n",
       " (['i', 'must', 'admit', 'that', 'i', 'was', 'a', 'tad', ...], 'pos')]"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(documents)\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J') :\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V') :\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N') :\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R') :\n",
    "        return wordnet.ADV\n",
    "    else :\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning stop words and lemmatizing to find the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            ## To lemmatize the word, we need to pass the pos, here from tuple we want the first entry\n",
    "            clean_word = lemmatizer.lemmatize(w , pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = [(clean_review(document),category) for document, category in documents]"
   ]
  },
  {
   "source": [
    "# Splitting data into train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = documents[0:1500]\n",
    "testing_documents = documents[1500:]"
   ]
  },
  {
   "source": [
    "# Preparing data as NLTK classifier require data in this format\n",
    "Format is array of tuples , where each tuple has a dictionary which is feature and feature value(T/F) and the category"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "# Example\n",
    "a = [1,2]\n",
    "b = [3,4]\n",
    "a += b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for doc in training_documents:\n",
    "    all_words += doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(all_words) ## gives a frequency distribution object\n",
    "common = freq.most_common(3000)\n",
    "\n",
    "features = [i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dict(words):\n",
    "    current_features = {}\n",
    "    words_set = set(words)\n",
    "    for w in features :\n",
    "        current_features[w] = w in words_set\n",
    "    return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(get_features_dict(doc),category) for doc, category in training_documents ] ## array of tuple of dict and category\n",
    "testing_data = [(get_features_dict(doc),category) for doc, category in testing_documents ]"
   ]
  },
  {
   "source": [
    "# NLTK "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier  = NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "nltk.classify.accuracy(classfier, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most Informative Features\n             outstanding = True              pos : neg    =     13.7 : 1.0\n               ludicrous = True              neg : pos    =     12.0 : 1.0\n                 idiotic = True              neg : pos    =     11.6 : 1.0\n                  seagal = True              neg : pos    =     10.9 : 1.0\n                  sloppy = True              neg : pos    =     10.8 : 1.0\n              schumacher = True              neg : pos    =     10.2 : 1.0\n                   anger = True              pos : neg    =      9.1 : 1.0\n                    coen = True              pos : neg    =      8.5 : 1.0\n            breathtaking = True              pos : neg    =      8.3 : 1.0\n             magnificent = True              pos : neg    =      7.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classfier.show_most_informative_features()"
   ]
  },
  {
   "source": [
    "# SKLearn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "classifier_sklearn = SklearnClassifier(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))>"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "classifier_sklearn.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.846"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "classifier_sklearn1 = SklearnClassifier(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<SklearnClassifier(RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False))>"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "classifier_sklearn1.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn1, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}